{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGNet Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from sgnet import *\n",
    "from generator.generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = x.repeat(1, 4)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_config():\n",
    "    return {\n",
    "        'base_lr': 0.0005,\n",
    "        'num_data_loaders' : 4,     \n",
    "        'num_training_epochs' : 20,\n",
    "        'batch_size' : 32,\n",
    "        'pin_memory': True,\n",
    "        'use_apex' : False,\n",
    "        'warmup_period_for_lr' : 1000000, \n",
    "        'decay_interval' : 0,\n",
    "        'decay' : 0.8,\n",
    "        'l2_reg' : 1e-4,\n",
    "        'resource_path': 'J:\\\\datasets\\\\faze-resources',\n",
    "        'npz_root_path': 'J:\\\\datasets\\\\everyone-recode',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_mock_config()\n",
    "Context.build(config)\n",
    "ctx = Context.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_key = \"ss\"\n",
    "INPUT_SIZE = (3, 64, 64)\n",
    "network = SGNet(INPUT_SIZE, network_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.set_network(network_key, network)\n",
    "ctx.load_network_to_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Base Loss Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-26 19:27:27,693 \n",
      "2020-01-26 19:27:27,696 >>> base lose policy <<<\n",
      "2020-01-26 19:27:27,697 max learning rate: 0.016000\n",
      "2020-01-26 19:27:27,698 ramp up a: 0.000000, b: 0.000500\n"
     ]
    }
   ],
   "source": [
    "ctx.setup_base_loss_policy(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-26 19:27:27,704 Set optimizer as key: optimizer\n"
     ]
    }
   ],
   "source": [
    "optimizer = ctx.build_optimizer(network_key)\n",
    "ctx.set_main_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-26 19:27:27,711 \n",
      "2020-01-26 19:27:27,712 >>> Setup loss functions <<<\n",
      "2020-01-26 19:27:27,713 list: { gaze } \n"
     ]
    }
   ],
   "source": [
    "loss_functions = ctx.make_loss_functions()\n",
    "ctx.set_loss_functions(loss_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-26 19:27:37,465 \n",
      "2020-01-26 19:27:37,466 >>> data-bag is setup\n",
      "2020-01-26 19:27:37,467 => { gc/train, gc/val, gc/test } \n"
     ]
    }
   ],
   "source": [
    "def selector(self, idx):\n",
    "    profile_id, item_idx = self.index_to_query[idx]    \n",
    "    profile_idx = self.profiles.index(\"{:05d}\".format(profile_id))\n",
    "    npz = self.load_recode_profile_npz(self.npz_root_path, profile_id)\n",
    "\n",
    "# gen = NPZDatasetGenerator(ctx, shuffle_train=False, item_selector_fn=selector)\n",
    "gen = NPZDatasetGenerator(ctx, shuffle_train=False)\n",
    "databag = gen.generate(verbose=False)\n",
    "ctx.set_databag(databag)\n",
    "\n",
    "train_ds, loader = ctx.get_dataset_and_loader(Context.TAG_TRAIN)\n",
    "# train_ds.index_to_query[1900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx.use_multiple_gpu_if_available()\n",
    "# ctx.run_train(network_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_training_step(current_step)\n",
    "\n",
    "# if current_step % args.print_freq_train == args.print_freq_train - 1:\n",
    "#     conv1_wt_lr = optimizer.param_groups[0]['lr']\n",
    "#     running_loss_means = running_losses.means()\n",
    "#     logging.info('Losses at [%7d]: %s' %\n",
    "#                  (current_step + 1,\n",
    "#                   ', '.join(['%s: %.5f' % v\n",
    "#                              for v in running_loss_means.items()])))\n",
    "#     if args.use_tensorboard:\n",
    "#         tensorboard.add_scalar('train_lr', conv1_wt_lr, current_step + 1)\n",
    "#         for k, v in running_loss_means.items():\n",
    "#             tensorboard.add_scalar('train/' + k, v, current_step + 1)\n",
    "#     running_losses.reset()\n",
    "\n",
    "# # Print some timing statistics\n",
    "# if current_step % 100 == 99:\n",
    "#     if args.use_tensorboard:\n",
    "#         for k, v in running_timings.means().items():\n",
    "#             tensorboard.add_scalar('timing/' + k, v, current_step + 1)\n",
    "#     running_timings.reset()\n",
    "\n",
    "# # print some memory statistics\n",
    "# if current_step % 5000 == 0:\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         bytes = (torch.cuda.memory_allocated(device=i)\n",
    "#                  + torch.cuda.memory_cached(device=i))\n",
    "#         logging.info('GPU %d: probably allocated approximately %.2f GB' % (i, bytes / 1e9))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
