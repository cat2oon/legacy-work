{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from modelsummary import summary\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "                 Conv2d-1            [-1, 3, 64, 64]             432\n",
      "            BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "                   ReLU-3           [-1, 16, 32, 32]               0\n",
      "                 Conv2d-4           [-1, 16, 32, 32]           4,608\n",
      "=======================================================================\n",
      "Total params: 5,072\n",
      "Trainable params: 5,072\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "next channel: 32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetInitialLayers\n",
    "\"\"\"\n",
    "class DenseNetInitialLayers(nn.Module):\n",
    "    def __init__(self, \n",
    "                 growth_rate=8, \n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d):\n",
    "        super(DenseNetInitialLayers, self).__init__()\n",
    "        \n",
    "        c_next = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, c_next, bias=False, kernel_size=3, stride=2, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight.data)\n",
    "\n",
    "        self.norm = normalization_fn(c_next, track_running_stats=False).to(device)\n",
    "        self.act = activation_fn(inplace=True)\n",
    "\n",
    "        c_out = 4 * growth_rate\n",
    "        self.conv2 = nn.Conv2d(2 * growth_rate, c_out, bias=False, kernel_size=3, stride=2, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight.data)\n",
    "\n",
    "        self.c_now = c_out\n",
    "        self.c_list = [c_next, c_out]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        prev_scale_x = x\n",
    "        x = self.conv2(x)\n",
    "        return x, prev_scale_x\n",
    "\n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "net_init_layer = DenseNetInitialLayers()\n",
    "net_init_layer.to(device)\n",
    "data = torch.zeros((16, 3, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(net_init_layer, data)\n",
    "\n",
    "c_now = net_init_layer.c_now\n",
    "print(\"next channel:\", c_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "            BatchNorm2d-1           [-1, 32, 64, 64]              64\n",
      "                   ReLU-2           [-1, 32, 64, 64]               0\n",
      "                 Conv2d-3           [-1, 32, 64, 64]           9,216\n",
      "              Dropout2d-4           [-1, 32, 64, 64]               0\n",
      "=======================================================================\n",
      "Total params: 9,280\n",
      "Trainable params: 9,280\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetCompositeLayer\n",
    "\"\"\"\n",
    "class DenseNetCompositeLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 c_out, \n",
    "                 kernel_size=3, \n",
    "                 growth_rate=8, \n",
    "                 p_dropout=0.1,\n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d,\n",
    "                 transposed=False):\n",
    "        super(DenseNetCompositeLayer, self).__init__()\n",
    "        \n",
    "        self.norm = normalization_fn(c_in, track_running_stats=False).to(device)\n",
    "        self.act = activation_fn(inplace=True)\n",
    "        \n",
    "        if transposed:\n",
    "            assert kernel_size > 1\n",
    "            conv_layer = nn.ConvTranspose2d\n",
    "        else:\n",
    "            conv_layer = nn.Conv2d\n",
    "        \n",
    "        self.conv = conv_layer(c_in, \n",
    "                               c_out, \n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=1 if kernel_size > 1 else 0,\n",
    "                               stride=1, \n",
    "                               bias=False).to(device)\n",
    "            \n",
    "        nn.init.kaiming_normal_(self.conv.weight.data)\n",
    "        self.drop = nn.Dropout2d(p=p_dropout) if p_dropout > 1e-5 else None\n",
    "        self.c_now = c_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        if self.drop is not None:\n",
    "            x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "net_comp_layer = DenseNetCompositeLayer(c_in=c_now, c_out=4 * 8)  # c_out = 4 * growth_rate\n",
    "net_comp_layer.to(device)\n",
    "data = torch.zeros((16, c_now, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(net_comp_layer, data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      " DenseNetCompositeLayer-1           [-1, 32, 64, 64]               0\n",
      "            BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "                   ReLU-3           [-1, 32, 64, 64]               0\n",
      "                 Conv2d-4           [-1, 32, 64, 64]           2,304\n",
      "              Dropout2d-5            [-1, 8, 64, 64]               0\n",
      " DenseNetCompositeLayer-6           [-1, 40, 64, 64]               0\n",
      "            BatchNorm2d-7           [-1, 40, 64, 64]              80\n",
      "                   ReLU-8           [-1, 40, 64, 64]               0\n",
      "                 Conv2d-9           [-1, 40, 64, 64]           2,880\n",
      "             Dropout2d-10            [-1, 8, 64, 64]               0\n",
      "DenseNetCompositeLayer-11           [-1, 48, 64, 64]               0\n",
      "           BatchNorm2d-12           [-1, 48, 64, 64]              96\n",
      "                  ReLU-13           [-1, 48, 64, 64]               0\n",
      "                Conv2d-14           [-1, 48, 64, 64]           3,456\n",
      "             Dropout2d-15            [-1, 8, 64, 64]               0\n",
      "DenseNetCompositeLayer-16           [-1, 56, 64, 64]               0\n",
      "           BatchNorm2d-17           [-1, 56, 64, 64]             112\n",
      "                  ReLU-18           [-1, 56, 64, 64]               0\n",
      "                Conv2d-19           [-1, 56, 64, 64]           4,032\n",
      "             Dropout2d-20            [-1, 8, 64, 64]               0\n",
      "=======================================================================\n",
      "Total params: 13,024\n",
      "Trainable params: 13,024\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetBlock\n",
    "\"\"\"\n",
    "class DenseNetBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 num_layers=4, \n",
    "                 growth_rate=8, \n",
    "                 p_dropout=0.1,\n",
    "                 use_bottleneck=False, \n",
    "                 activation_fn=nn.ReLU,\n",
    "                 normalization_fn=nn.BatchNorm2d, \n",
    "                 transposed=False):\n",
    "        super(DenseNetBlock, self).__init__()\n",
    "        \n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        c_now = c_in\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            i_ = i + 1\n",
    "            if use_bottleneck:\n",
    "                bottleneck = DenseNetCompositeLayer(c_now, \n",
    "                                                    4 * growth_rate, \n",
    "                                                    kernel_size=1, \n",
    "                                                    p_dropout=p_dropout,\n",
    "                                                    activation_fn=activation_fn,\n",
    "                                                    normalization_fn=normalization_fn)\n",
    "                self.add_module('bneck%d' % i_, bottleneck)\n",
    "            \n",
    "            c_in_compos = 4 * growth_rate if use_bottleneck else c_now\n",
    "            composit_layer = DenseNetCompositeLayer(c_in_compos, \n",
    "                                                    growth_rate,\n",
    "                                                    kernel_size=3, \n",
    "                                                    p_dropout=p_dropout,\n",
    "                                                    activation_fn=activation_fn,\n",
    "                                                    normalization_fn=normalization_fn,\n",
    "                                                    transposed=transposed)\n",
    "            self.add_module('compo%d' % i_, composit_layer)\n",
    "            c_now += list(self.children())[-1].c_now\n",
    "        self.c_now = c_now\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_before = x\n",
    "        for i, (name, module) in enumerate(self.named_children()):\n",
    "            if ((self.use_bottleneck and name.startswith('bneck')) or name.startswith('compo')):\n",
    "                x_before = x\n",
    "            x = module(x)\n",
    "            if name.startswith('compo'):\n",
    "                x = torch.cat([x_before, x], dim=1)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "net_block = DenseNetBlock(c_in=c_now)\n",
    "net_block.to(device)\n",
    "data = torch.zeros((16, c_now, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(net_block, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      " DenseNetCompositeLayer-1           [-1, 32, 64, 64]               0\n",
      "            BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "                   ReLU-3           [-1, 32, 64, 64]               0\n",
      "                 Conv2d-4           [-1, 32, 64, 64]              96\n",
      "              Dropout2d-5            [-1, 3, 64, 64]               0\n",
      "              AvgPool2d-6            [-1, 3, 64, 64]               0\n",
      "=======================================================================\n",
      "Total params: 160\n",
      "Trainable params: 160\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetTransitionDown\n",
    "\"\"\"\n",
    "class DenseNetTransitionDown(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 compression_factor=0.1, \n",
    "                 p_dropout=0.1,\n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d):\n",
    "        super(DenseNetTransitionDown, self).__init__()\n",
    "        \n",
    "        c_out = int(compression_factor * c_in)\n",
    "        self.composite = DenseNetCompositeLayer(c_in, \n",
    "                                                c_out,         \n",
    "                                                kernel_size=1, \n",
    "                                                p_dropout=p_dropout,            \n",
    "                                                activation_fn=activation_fn,\n",
    "                                                normalization_fn=normalization_fn)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.c_now = c_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.composite(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "trans_block = DenseNetTransitionDown(c_in=c_now)\n",
    "trans_block.to(device)\n",
    "data = torch.zeros((16, c_now, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(trans_block, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "            BatchNorm2d-1           [-1, 32, 64, 64]              64\n",
      "                   ReLU-2           [-1, 32, 64, 64]               0\n",
      "        ConvTranspose2d-3           [-1, 32, 64, 64]             864\n",
      "=======================================================================\n",
      "Total params: 928\n",
      "Trainable params: 928\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetTransitionUp\n",
    "\"\"\"\n",
    "class DenseNetTransitionUp(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 compression_factor=0.1, \n",
    "                 p_dropout=0.1,\n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d):\n",
    "        super(DenseNetTransitionUp, self).__init__()\n",
    "        \n",
    "        c_out = int(compression_factor * c_in)\n",
    "        self.norm = normalization_fn(c_in, track_running_stats=False).to(device)\n",
    "        self.act = activation_fn(inplace=True)\n",
    "        self.conv = nn.ConvTranspose2d(c_in, \n",
    "                                       c_out, \n",
    "                                       kernel_size=3,\n",
    "                                       stride=2, \n",
    "                                       padding=1, \n",
    "                                       output_padding=1,\n",
    "                                       bias=False).to(device)\n",
    "        nn.init.kaiming_normal_(self.conv.weight.data)\n",
    "        self.drop = nn.Dropout2d(p=p_dropout) if p_dropout > 1e-5 else None\n",
    "        self.c_now = c_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "trans_block = DenseNetTransitionUp(c_in=c_now)\n",
    "trans_block.to(device)\n",
    "data = torch.zeros((16, c_now, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(trans_block, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "        ConvTranspose2d-1           [-1, 32, 64, 64]           9,216\n",
      "            BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "                   ReLU-3         [-1, 32, 128, 128]               0\n",
      "        ConvTranspose2d-4         [-1, 32, 128, 128]           4,608\n",
      "            BatchNorm2d-5         [-1, 16, 256, 256]              32\n",
      "                   ReLU-6         [-1, 16, 256, 256]               0\n",
      "                 Conv2d-7         [-1, 16, 256, 256]             432\n",
      "=======================================================================\n",
      "Total params: 14,352\n",
      "Trainable params: 14,352\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetDecoderLastLayers\n",
    "\"\"\"\n",
    "class DenseNetDecoderLastLayers(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 growth_rate=8, \n",
    "                 activation_fn=nn.ReLU,\n",
    "                 normalization_fn=nn.BatchNorm2d,\n",
    "                 skip_connection_growth=0):\n",
    "        super(DenseNetDecoderLastLayers, self).__init__()\n",
    "        \n",
    "        # First deconv\n",
    "        self.conv1 = nn.ConvTranspose2d(c_in, \n",
    "                                        4 * growth_rate, \n",
    "                                        bias=False,\n",
    "                                        kernel_size=3, \n",
    "                                        stride=2, \n",
    "                                        padding=1,\n",
    "                                        output_padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight.data)\n",
    "\n",
    "        # Second deconv\n",
    "        c_in = 4 * growth_rate + skip_connection_growth\n",
    "        self.norm2 = normalization_fn(c_in, track_running_stats=False).to(device)\n",
    "        self.act = activation_fn(inplace=True)\n",
    "        self.conv2 = nn.ConvTranspose2d(c_in, \n",
    "                                        2 * growth_rate, \n",
    "                                        bias=False,\n",
    "                                        kernel_size=3, \n",
    "                                        stride=2, \n",
    "                                        padding=1,\n",
    "                                        output_padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight.data)\n",
    "\n",
    "        # Final conv\n",
    "        c_in = 2 * growth_rate\n",
    "        c_out = 3\n",
    "        self.norm3 = normalization_fn(c_in, track_running_stats=False).to(device)\n",
    "        self.conv3 = nn.Conv2d(c_in, \n",
    "                               c_out, \n",
    "                               bias=False,\n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight.data)\n",
    "        self.c_now = c_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        #\n",
    "        x = self.norm3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "deconv = DenseNetDecoderLastLayers(c_in=c_now)\n",
    "deconv.to(device)\n",
    "data = torch.zeros((16, c_now, 64, 64))\n",
    "data.to(device)\n",
    "\n",
    "summary(deconv, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "  DenseNetInitialLayers-1          [-1, 3, 224, 224]               0\n",
      "                 Conv2d-2          [-1, 3, 224, 224]             432\n",
      "            BatchNorm2d-3         [-1, 16, 112, 112]              32\n",
      "                   ReLU-4         [-1, 16, 112, 112]               0\n",
      "                 Conv2d-5         [-1, 16, 112, 112]           4,608\n",
      "          DenseNetBlock-6           [-1, 32, 56, 56]               0\n",
      " DenseNetCompositeLayer-7           [-1, 32, 56, 56]               0\n",
      "            BatchNorm2d-8           [-1, 32, 56, 56]              64\n",
      "                   ReLU-9           [-1, 32, 56, 56]               0\n",
      "                Conv2d-10           [-1, 32, 56, 56]           2,304\n",
      "DenseNetCompositeLayer-11           [-1, 40, 56, 56]               0\n",
      "           BatchNorm2d-12           [-1, 40, 56, 56]              80\n",
      "                  ReLU-13           [-1, 40, 56, 56]               0\n",
      "                Conv2d-14           [-1, 40, 56, 56]           2,880\n",
      "DenseNetCompositeLayer-15           [-1, 48, 56, 56]               0\n",
      "           BatchNorm2d-16           [-1, 48, 56, 56]              96\n",
      "                  ReLU-17           [-1, 48, 56, 56]               0\n",
      "                Conv2d-18           [-1, 48, 56, 56]           3,456\n",
      "DenseNetCompositeLayer-19           [-1, 56, 56, 56]               0\n",
      "           BatchNorm2d-20           [-1, 56, 56, 56]             112\n",
      "                  ReLU-21           [-1, 56, 56, 56]               0\n",
      "                Conv2d-22           [-1, 56, 56, 56]           4,032\n",
      "DenseNetTransitionDown-23           [-1, 64, 56, 56]               0\n",
      "DenseNetCompositeLayer-24           [-1, 64, 56, 56]               0\n",
      "           BatchNorm2d-25           [-1, 64, 56, 56]             128\n",
      "                  ReLU-26           [-1, 64, 56, 56]               0\n",
      "                Conv2d-27           [-1, 64, 56, 56]           4,096\n",
      "             AvgPool2d-28           [-1, 64, 56, 56]               0\n",
      "         DenseNetBlock-29           [-1, 64, 28, 28]               0\n",
      "DenseNetCompositeLayer-30           [-1, 64, 28, 28]               0\n",
      "           BatchNorm2d-31           [-1, 64, 28, 28]             128\n",
      "                  ReLU-32           [-1, 64, 28, 28]               0\n",
      "                Conv2d-33           [-1, 64, 28, 28]           4,608\n",
      "DenseNetCompositeLayer-34           [-1, 72, 28, 28]               0\n",
      "           BatchNorm2d-35           [-1, 72, 28, 28]             144\n",
      "                  ReLU-36           [-1, 72, 28, 28]               0\n",
      "                Conv2d-37           [-1, 72, 28, 28]           5,184\n",
      "DenseNetCompositeLayer-38           [-1, 80, 28, 28]               0\n",
      "           BatchNorm2d-39           [-1, 80, 28, 28]             160\n",
      "                  ReLU-40           [-1, 80, 28, 28]               0\n",
      "                Conv2d-41           [-1, 80, 28, 28]           5,760\n",
      "DenseNetCompositeLayer-42           [-1, 88, 28, 28]               0\n",
      "           BatchNorm2d-43           [-1, 88, 28, 28]             176\n",
      "                  ReLU-44           [-1, 88, 28, 28]               0\n",
      "                Conv2d-45           [-1, 88, 28, 28]           6,336\n",
      "DenseNetTransitionDown-46           [-1, 96, 28, 28]               0\n",
      "DenseNetCompositeLayer-47           [-1, 96, 28, 28]               0\n",
      "           BatchNorm2d-48           [-1, 96, 28, 28]             192\n",
      "                  ReLU-49           [-1, 96, 28, 28]               0\n",
      "                Conv2d-50           [-1, 96, 28, 28]           9,216\n",
      "             AvgPool2d-51           [-1, 96, 28, 28]               0\n",
      "         DenseNetBlock-52           [-1, 96, 14, 14]               0\n",
      "DenseNetCompositeLayer-53           [-1, 96, 14, 14]               0\n",
      "           BatchNorm2d-54           [-1, 96, 14, 14]             192\n",
      "                  ReLU-55           [-1, 96, 14, 14]               0\n",
      "                Conv2d-56           [-1, 96, 14, 14]           6,912\n",
      "DenseNetCompositeLayer-57          [-1, 104, 14, 14]               0\n",
      "           BatchNorm2d-58          [-1, 104, 14, 14]             208\n",
      "                  ReLU-59          [-1, 104, 14, 14]               0\n",
      "                Conv2d-60          [-1, 104, 14, 14]           7,488\n",
      "DenseNetCompositeLayer-61          [-1, 112, 14, 14]               0\n",
      "           BatchNorm2d-62          [-1, 112, 14, 14]             224\n",
      "                  ReLU-63          [-1, 112, 14, 14]               0\n",
      "                Conv2d-64          [-1, 112, 14, 14]           8,064\n",
      "DenseNetCompositeLayer-65          [-1, 120, 14, 14]               0\n",
      "           BatchNorm2d-66          [-1, 120, 14, 14]             240\n",
      "                  ReLU-67          [-1, 120, 14, 14]               0\n",
      "                Conv2d-68          [-1, 120, 14, 14]           8,640\n",
      "DenseNetTransitionDown-69          [-1, 128, 14, 14]               0\n",
      "DenseNetCompositeLayer-70          [-1, 128, 14, 14]               0\n",
      "           BatchNorm2d-71          [-1, 128, 14, 14]             256\n",
      "                  ReLU-72          [-1, 128, 14, 14]               0\n",
      "                Conv2d-73          [-1, 128, 14, 14]          16,384\n",
      "             AvgPool2d-74          [-1, 128, 14, 14]               0\n",
      "         DenseNetBlock-75            [-1, 128, 7, 7]               0\n",
      "DenseNetCompositeLayer-76            [-1, 128, 7, 7]               0\n",
      "           BatchNorm2d-77            [-1, 128, 7, 7]             256\n",
      "                  ReLU-78            [-1, 128, 7, 7]               0\n",
      "                Conv2d-79            [-1, 128, 7, 7]           9,216\n",
      "DenseNetCompositeLayer-80            [-1, 136, 7, 7]               0\n",
      "           BatchNorm2d-81            [-1, 136, 7, 7]             272\n",
      "                  ReLU-82            [-1, 136, 7, 7]               0\n",
      "                Conv2d-83            [-1, 136, 7, 7]           9,792\n",
      "DenseNetCompositeLayer-84            [-1, 144, 7, 7]               0\n",
      "           BatchNorm2d-85            [-1, 144, 7, 7]             288\n",
      "                  ReLU-86            [-1, 144, 7, 7]               0\n",
      "                Conv2d-87            [-1, 144, 7, 7]          10,368\n",
      "DenseNetCompositeLayer-88            [-1, 152, 7, 7]               0\n",
      "           BatchNorm2d-89            [-1, 152, 7, 7]             304\n",
      "                  ReLU-90            [-1, 152, 7, 7]               0\n",
      "                Conv2d-91            [-1, 152, 7, 7]          10,944\n",
      "=======================================================================\n",
      "Total params: 144,272\n",
      "Trainable params: 144,272\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetEncoder\n",
    "\"\"\"\n",
    "class DenseNetEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 growth_rate=8, \n",
    "                 num_blocks=4, \n",
    "                 num_layers_per_block=4,\n",
    "                 p_dropout=0.0, \n",
    "                 compression_factor=1.0,\n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d):\n",
    "        super(DenseNetEncoder, self).__init__()\n",
    "        \n",
    "        self.c_at_end_of_each_scale = []\n",
    "\n",
    "        # Initial down-sampling conv layers\n",
    "        self.initial = DenseNetInitialLayers(growth_rate=growth_rate,\n",
    "                                             activation_fn=activation_fn,\n",
    "                                             normalization_fn=normalization_fn)\n",
    "        c_now = list(self.children())[-1].c_now\n",
    "        self.c_at_end_of_each_scale += list(self.children())[-1].c_list\n",
    "\n",
    "        assert (num_layers_per_block % 2) == 0\n",
    "        for i in range(num_blocks):\n",
    "            i_ = i + 1\n",
    "            dense_block = DenseNetBlock(c_now,\n",
    "                                       num_layers=num_layers_per_block,\n",
    "                                       growth_rate=growth_rate,                       \n",
    "                                       p_dropout=p_dropout,\n",
    "                                       activation_fn=activation_fn,\n",
    "                                       normalization_fn=normalization_fn)\n",
    "            self.add_module('block%d' % i_, dense_block)\n",
    "            c_now = list(self.children())[-1].c_now\n",
    "            self.c_at_end_of_each_scale.append(c_now)\n",
    "\n",
    "            if i < (num_blocks - 1):  # transition block if not last layer\n",
    "                dense_trans_down = DenseNetTransitionDown(c_now,\n",
    "                                                          p_dropout=p_dropout,\n",
    "                                                          compression_factor=compression_factor,\n",
    "                                                          activation_fn=activation_fn,\n",
    "                                                          normalization_fn=normalization_fn)\n",
    "                self.add_module('trans%d' % i_, dense_trans_down)\n",
    "                c_now = list(self.children())[-1].c_now\n",
    "            self.c_now = c_now\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply initial layers and dense blocks\n",
    "        for name, module in self.named_children():\n",
    "            if name == 'initial':\n",
    "                x, prev_scale_x = module(x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "dense_encoder = DenseNetEncoder()\n",
    "dense_encoder.to(device)\n",
    "data = torch.zeros((16, 3, 224, 224))\n",
    "data.to(device)\n",
    "\n",
    "summary(dense_encoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "          DenseNetBlock-1              [-1, 3, 2, 8]               0\n",
      " DenseNetCompositeLayer-2              [-1, 3, 2, 8]               0\n",
      "            BatchNorm2d-3              [-1, 3, 2, 8]               6\n",
      "                   ReLU-4              [-1, 3, 2, 8]               0\n",
      "        ConvTranspose2d-5              [-1, 3, 2, 8]             216\n",
      " DenseNetCompositeLayer-6             [-1, 11, 2, 8]               0\n",
      "            BatchNorm2d-7             [-1, 11, 2, 8]              22\n",
      "                   ReLU-8             [-1, 11, 2, 8]               0\n",
      "        ConvTranspose2d-9             [-1, 11, 2, 8]             792\n",
      "DenseNetCompositeLayer-10             [-1, 19, 2, 8]               0\n",
      "           BatchNorm2d-11             [-1, 19, 2, 8]              38\n",
      "                  ReLU-12             [-1, 19, 2, 8]               0\n",
      "       ConvTranspose2d-13             [-1, 19, 2, 8]           1,368\n",
      "DenseNetCompositeLayer-14             [-1, 27, 2, 8]               0\n",
      "           BatchNorm2d-15             [-1, 27, 2, 8]              54\n",
      "                  ReLU-16             [-1, 27, 2, 8]               0\n",
      "       ConvTranspose2d-17             [-1, 27, 2, 8]           1,944\n",
      "  DenseNetTransitionUp-18             [-1, 35, 2, 8]               0\n",
      "           BatchNorm2d-19             [-1, 35, 2, 8]              70\n",
      "                  ReLU-20             [-1, 35, 2, 8]               0\n",
      "       ConvTranspose2d-21             [-1, 35, 2, 8]          11,025\n",
      "         DenseNetBlock-22            [-1, 35, 4, 16]               0\n",
      "DenseNetCompositeLayer-23            [-1, 35, 4, 16]               0\n",
      "           BatchNorm2d-24            [-1, 35, 4, 16]              70\n",
      "                  ReLU-25            [-1, 35, 4, 16]               0\n",
      "       ConvTranspose2d-26            [-1, 35, 4, 16]           2,520\n",
      "DenseNetCompositeLayer-27            [-1, 43, 4, 16]               0\n",
      "           BatchNorm2d-28            [-1, 43, 4, 16]              86\n",
      "                  ReLU-29            [-1, 43, 4, 16]               0\n",
      "       ConvTranspose2d-30            [-1, 43, 4, 16]           3,096\n",
      "DenseNetCompositeLayer-31            [-1, 51, 4, 16]               0\n",
      "           BatchNorm2d-32            [-1, 51, 4, 16]             102\n",
      "                  ReLU-33            [-1, 51, 4, 16]               0\n",
      "       ConvTranspose2d-34            [-1, 51, 4, 16]           3,672\n",
      "DenseNetCompositeLayer-35            [-1, 59, 4, 16]               0\n",
      "           BatchNorm2d-36            [-1, 59, 4, 16]             118\n",
      "                  ReLU-37            [-1, 59, 4, 16]               0\n",
      "       ConvTranspose2d-38            [-1, 59, 4, 16]           4,248\n",
      "  DenseNetTransitionUp-39            [-1, 67, 4, 16]               0\n",
      "           BatchNorm2d-40            [-1, 67, 4, 16]             134\n",
      "                  ReLU-41            [-1, 67, 4, 16]               0\n",
      "       ConvTranspose2d-42            [-1, 67, 4, 16]          40,401\n",
      "         DenseNetBlock-43            [-1, 67, 8, 32]               0\n",
      "DenseNetCompositeLayer-44            [-1, 67, 8, 32]               0\n",
      "           BatchNorm2d-45            [-1, 67, 8, 32]             134\n",
      "                  ReLU-46            [-1, 67, 8, 32]               0\n",
      "       ConvTranspose2d-47            [-1, 67, 8, 32]           4,824\n",
      "DenseNetCompositeLayer-48            [-1, 75, 8, 32]               0\n",
      "           BatchNorm2d-49            [-1, 75, 8, 32]             150\n",
      "                  ReLU-50            [-1, 75, 8, 32]               0\n",
      "       ConvTranspose2d-51            [-1, 75, 8, 32]           5,400\n",
      "DenseNetCompositeLayer-52            [-1, 83, 8, 32]               0\n",
      "           BatchNorm2d-53            [-1, 83, 8, 32]             166\n",
      "                  ReLU-54            [-1, 83, 8, 32]               0\n",
      "       ConvTranspose2d-55            [-1, 83, 8, 32]           5,976\n",
      "DenseNetCompositeLayer-56            [-1, 91, 8, 32]               0\n",
      "           BatchNorm2d-57            [-1, 91, 8, 32]             182\n",
      "                  ReLU-58            [-1, 91, 8, 32]               0\n",
      "       ConvTranspose2d-59            [-1, 91, 8, 32]           6,552\n",
      "  DenseNetTransitionUp-60            [-1, 99, 8, 32]               0\n",
      "           BatchNorm2d-61            [-1, 99, 8, 32]             198\n",
      "                  ReLU-62            [-1, 99, 8, 32]               0\n",
      "       ConvTranspose2d-63            [-1, 99, 8, 32]          88,209\n",
      "         DenseNetBlock-64           [-1, 99, 16, 64]               0\n",
      "DenseNetCompositeLayer-65           [-1, 99, 16, 64]               0\n",
      "           BatchNorm2d-66           [-1, 99, 16, 64]             198\n",
      "                  ReLU-67           [-1, 99, 16, 64]               0\n",
      "       ConvTranspose2d-68           [-1, 99, 16, 64]           7,128\n",
      "DenseNetCompositeLayer-69          [-1, 107, 16, 64]               0\n",
      "           BatchNorm2d-70          [-1, 107, 16, 64]             214\n",
      "                  ReLU-71          [-1, 107, 16, 64]               0\n",
      "       ConvTranspose2d-72          [-1, 107, 16, 64]           7,704\n",
      "DenseNetCompositeLayer-73          [-1, 115, 16, 64]               0\n",
      "           BatchNorm2d-74          [-1, 115, 16, 64]             230\n",
      "                  ReLU-75          [-1, 115, 16, 64]               0\n",
      "       ConvTranspose2d-76          [-1, 115, 16, 64]           8,280\n",
      "DenseNetCompositeLayer-77          [-1, 123, 16, 64]               0\n",
      "           BatchNorm2d-78          [-1, 123, 16, 64]             246\n",
      "                  ReLU-79          [-1, 123, 16, 64]               0\n",
      "       ConvTranspose2d-80          [-1, 123, 16, 64]           8,856\n",
      "DenseNetDecoderLastLayers-81          [-1, 131, 16, 64]               0\n",
      "       ConvTranspose2d-82          [-1, 131, 16, 64]          37,728\n",
      "           BatchNorm2d-83          [-1, 32, 32, 128]              64\n",
      "                  ReLU-84          [-1, 32, 32, 128]               0\n",
      "       ConvTranspose2d-85          [-1, 32, 32, 128]           4,608\n",
      "           BatchNorm2d-86          [-1, 16, 64, 256]              32\n",
      "                  ReLU-87          [-1, 16, 64, 256]               0\n",
      "                Conv2d-88          [-1, 16, 64, 256]             432\n",
      "=======================================================================\n",
      "Total params: 257,493\n",
      "Trainable params: 257,493\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DenseNetDecoder\n",
    "\"\"\"\n",
    "class DenseNetDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 growth_rate=8, \n",
    "                 num_blocks=4, \n",
    "                 num_layers_per_block=4,\n",
    "                 p_dropout=0.0, \n",
    "                 compression_factor=1.0,\n",
    "                 activation_fn=nn.ReLU, \n",
    "                 normalization_fn=nn.BatchNorm2d,\n",
    "                 use_skip_connections_from=None):\n",
    "        super(DenseNetDecoder, self).__init__()\n",
    "\n",
    "        self.use_skip_connections = (use_skip_connections_from is not None)\n",
    "        if self.use_skip_connections:\n",
    "            c_to_concat = use_skip_connections_from.c_at_end_of_each_scale\n",
    "            c_to_concat = list(reversed(c_to_concat))[1:]\n",
    "        else:\n",
    "            c_to_concat = [0] * (num_blocks + 2)\n",
    "\n",
    "        assert (num_layers_per_block % 2) == 0\n",
    "        c_now = c_in\n",
    "        for i in range(num_blocks):\n",
    "            i_ = i + 1\n",
    "            dense_block = DenseNetBlock(c_now,\n",
    "                                        num_layers=num_layers_per_block,\n",
    "                                        growth_rate=growth_rate,\n",
    "                                        p_dropout=p_dropout,\n",
    "                                        activation_fn=activation_fn,\n",
    "                                        normalization_fn=normalization_fn,\n",
    "                                        transposed=True)\n",
    "            self.add_module('block%d' % i_, dense_block)\n",
    "            c_now = list(self.children())[-1].c_now\n",
    "            \n",
    "            if i < (num_blocks - 1):    # transn block if not last layer\n",
    "                dense_trans_up = DenseNetTransitionUp(c_now, \n",
    "                                                      p_dropout=p_dropout,\n",
    "                                                      compression_factor=compression_factor,\n",
    "                                                      activation_fn=activation_fn,\n",
    "                                                      normalization_fn=normalization_fn)\n",
    "                self.add_module('trans%d' % i_, dense_trans_up)\n",
    "                c_now = list(self.children())[-1].c_now\n",
    "                c_now += c_to_concat[i]\n",
    "\n",
    "        # Last up-sampling conv layers\n",
    "        self.last = DenseNetDecoderLastLayers(c_now,\n",
    "                                              growth_rate=growth_rate,\n",
    "                                              activation_fn=activation_fn,\n",
    "                                              normalization_fn=normalization_fn,\n",
    "                                              skip_connection_growth=c_to_concat[-1])\n",
    "        self.c_now = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply initial layers and dense blocks\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "dense_decoder = DenseNetDecoder(c_in=3)\n",
    "dense_decoder.to(device)\n",
    "data = torch.zeros((16, 3, 2, 8))        # w,h 크기에 따라 메모리 소모가 엄청 큼 (원래 bottleneck shape (2,8))\n",
    "data.to(device)\n",
    "\n",
    "summary(dense_decoder, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DTED\n",
    "\"\"\"\n",
    "class DTED(nn.Module):\n",
    "    def __init__(self, \n",
    "                 z_dim_app, \n",
    "                 z_dim_gaze, \n",
    "                 z_dim_head,\n",
    "                 growth_rate=32, \n",
    "                 activation_fn=nn.LeakyReLU,\n",
    "                 normalization_fn=nn.InstanceNorm2d,\n",
    "                 decoder_input_c=16,\n",
    "                 normalize_3d_codes=False,\n",
    "                 normalize_3d_codes_axis=None,\n",
    "                 use_triplet=False,\n",
    "                 gaze_hidden_layer_neurons=64,\n",
    "                 backprop_gaze_to_encoder=False):\n",
    "        super(DTED, self).__init__()\n",
    "\n",
    "        self.use_triplet = use_triplet\n",
    "        self.decoder_input_c = decoder_input_c\n",
    "        self.normalize_3d_codes = normalize_3d_codes\n",
    "        self.normalize_3d_codes_axis = normalize_3d_codes_axis\n",
    "        # self.gaze_hidden_layer_neurons = gaze_hidden_layer_neurons  # 아쒸. 인자로 해\n",
    "        self.backprop_gaze_to_encoder = backprop_gaze_to_encoder\n",
    "        if self.normalize_3d_codes:\n",
    "            assert self.normalize_3d_codes_axis is not None\n",
    "\n",
    "        # Define feature map dimensions at bottleneck\n",
    "        bottleneck_shape = (2, 8)\n",
    "        self.bottleneck_shape = bottleneck_shape\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = DenseNetEncoder(num_blocks=4,\n",
    "                                       growth_rate=growth_rate,\n",
    "                                       activation_fn=activation_fn,\n",
    "                                       normalization_fn=normalization_fn)\n",
    "        \n",
    "        c_now = list(self.children())[-1].c_now\n",
    "        enc_num_all = np.prod(bottleneck_shape) * decoder_input_c\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = DenseNetDecoder(decoder_input_c,\n",
    "                                       num_blocks=4,\n",
    "                                       growth_rate=growth_rate,\n",
    "                                       activation_fn=activation_fn,\n",
    "                                       normalization_fn=normalization_fn,\n",
    "                                       compression_factor=1.0)\n",
    "\n",
    "        # The latent code parts\n",
    "        self.z_dim_app = z_dim_app\n",
    "        self.z_dim_gaze = z_dim_gaze\n",
    "        self.z_dim_head = z_dim_head\n",
    "        z_num_all = 3 * (z_dim_gaze + z_dim_head) + z_dim_app\n",
    "\n",
    "        self.fc_enc = self.linear(c_now, z_num_all)\n",
    "        self.fc_dec = self.linear(z_num_all, enc_num_all)\n",
    "        self.build_gaze_layers(3 * z_dim_gaze, gaze_hidden_layer_neurons)\n",
    "\n",
    "    def build_gaze_layers(self, num_input_neurons, num_hidden_neurons=64):\n",
    "        self.gaze1 = self.linear(num_input_neurons, num_hidden_neurons)  # 1층\n",
    "        self.gaze2 = self.linear(num_hidden_neurons, 3)                  # 2층 \n",
    "\n",
    "    def linear(self, f_in, f_out):\n",
    "        fc = nn.Linear(f_in, f_out)\n",
    "        nn.init.kaiming_normal(fc.weight.data)\n",
    "        nn.init.constant(fc.bias.data, val=0)\n",
    "        return fc\n",
    "\n",
    "    def rotate_code(self, data, code, mode, fr=None, to=None):\n",
    "        \"\"\"\n",
    "        - Mode는 Gaze, Head 선택자\n",
    "        - \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" Must calculate transposed rotation matrices to be able to post-multiply to 3D codes \"\"\"\n",
    "        key_stem = 'R_' + mode\n",
    "        if fr is not None and to is not None:\n",
    "            rotate_mat = torch.matmul(data[key_stem + '_' + fr], torch.transpose(data[key_stem + '_' + to], 1, 2))\n",
    "        elif to is not None:\n",
    "            rotate_mat = torch.transpose(data[key_stem + '_' + to], 1, 2)\n",
    "        elif fr is not None:\n",
    "            # transpose-of-inverse is itself\n",
    "            rotate_mat = data[key_stem + '_' + fr]\n",
    "        return torch.matmul(code, rotate_mat)\n",
    "\n",
    "    def encode_to_z(self, data, suffix):\n",
    "        \"\"\"\n",
    "        - 이미지 페어에서 쌍 순서를 Suffix로 지정하고 인코더를 통해 latent 추출\n",
    "        - Suffix { a | b } -> 그냥 동일 프로파일 내의 임의의 이미지 x1, x2로 하면 안되나?\n",
    "           - 데이터셋 생성 과정에서 a, b 할당 정책 조사:\n",
    "           - park's preprocessing 과정과 관계없고 pytorch dataset (data.py)에서 할당\n",
    "        \"\"\"\n",
    "        x = self.encoder(data['image_' + suffix])\n",
    "        enc_output_shape = x.shape\n",
    "        x = x.mean(-1).mean(-1)  # Global-Average Pooling   # CHW에서 W, H에 대해 평균 내버림. 결과 (batch, channels)\n",
    "\n",
    "        \"\"\"\n",
    "        1) z_all = fc_enc(x) \n",
    "        - GAP 결과 채널수를 FC로 3 * (z_dim_gaze + z_dim_head) + z_dim_app 크기로 펼침 (batch, z_all)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create latent codes\n",
    "        z_all = self.fc_enc(x)\n",
    "        z_app = z_all[:, :self.z_dim_app]\n",
    "        z_all = z_all[:, self.z_dim_app:]\n",
    "        z_all = z_all.view(self.batch_size, -1, 3)   # 3배수 한 것대로 묶어주기\n",
    "        z_gaze_enc = z_all[:, :self.z_dim_gaze, :]   # GAZE\n",
    "        z_head_enc = z_all[:, self.z_dim_gaze:, :]   # HEAD\n",
    "\n",
    "        z_gaze_enc = z_gaze_enc.view(self.batch_size, -1, 3)\n",
    "        z_head_enc = z_head_enc.view(self.batch_size, -1, 3)\n",
    "        return [z_app, z_gaze_enc, z_head_enc, x, enc_output_shape]  # (z_a_a, ze1_g_a, ze1_h_a, ze1_before_z_a, _)\n",
    "\n",
    "    def decode_to_image(self, codes):\n",
    "        z_all = torch.cat([code.view(self.batch_size, -1) for code in codes], dim=1)\n",
    "        x = self.fc_dec(z_all)\n",
    "        x = x.view(self.batch_size, self.decoder_input_c, *self.bottleneck_shape)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def maybe_do_norm(self, code):\n",
    "        if self.normalize_3d_codes:\n",
    "            norm_axis = self.normalize_3d_codes_axis\n",
    "            assert code.dim() == 3\n",
    "            assert code.shape[-1] == 3\n",
    "            if norm_axis == 3:\n",
    "                b, f, _ = code.shape\n",
    "                code = code.view(b, -1)\n",
    "                normalized_code = F.normalize(code, dim=-1)\n",
    "                return normalized_code.view(b, f, -1)\n",
    "            else:\n",
    "                return F.normalize(code, dim=norm_axis)\n",
    "        return code\n",
    "\n",
    "    def forward(self, data, loss_functions=None):\n",
    "        is_inference_time = ('image_b' not in data)\n",
    "        self.batch_size = data['image_a'].shape[0]\n",
    "\n",
    "        \"\"\"\n",
    "        -> return of encode_to_z =>[z_app, z_gaze_enc, z_head_enc, x, enc_output_shape]\n",
    "        ze1_before_z_a 안쓰는 구만 왜 뽑냐 헷갈리게\n",
    "        \"\"\"\n",
    "        # Encode input from a \n",
    "        (z_a_a, ze1_g_a, ze1_h_a, ze1_before_z_a, _) = self.encode_to_z(data, 'a')\n",
    "        if not is_inference_time:\n",
    "            z_a_b, ze1_g_b, ze1_h_b, _, _ = self.encode_to_z(data, 'b')\n",
    "\n",
    "        \"\"\"\n",
    "        Latent 벡터 정규화 구에 임베딩 (x, y, z)이라 이렇게 해주면 당연히 좋을 거라 생각은 들지만\n",
    "        -> 논문에 이거에 대한 실험 결과 차이를 써주면 좋겠는데 없는 듯\n",
    "        \"\"\"\n",
    "        # Make each row a unit vector through L2 normalization to constrain\n",
    "        # embeddings to the surface of a hypersphere\n",
    "        if self.normalize_3d_codes:\n",
    "            assert ze1_g_a.dim() == ze1_h_a.dim() == 3\n",
    "            assert ze1_g_a.shape[-1] == ze1_h_a.shape[-1] == 3\n",
    "            ze1_g_a = self.maybe_do_norm(ze1_g_a)\n",
    "            ze1_h_a = self.maybe_do_norm(ze1_h_a)\n",
    "            if not is_inference_time:\n",
    "                ze1_g_b = self.maybe_do_norm(ze1_g_b)\n",
    "                ze1_h_b = self.maybe_do_norm(ze1_h_b)\n",
    "\n",
    "        \"\"\"\n",
    "        그래디언트 흘려 보낼지 결정\n",
    "        -> Loss에서 활용되는지 조사할 것:\n",
    "        -> 왜 클론 하는거야?? \n",
    "        \"\"\"\n",
    "        # Gaze estimation output for image a\n",
    "        if self.backprop_gaze_to_encoder:\n",
    "            gaze_features = ze1_g_a.clone().view(self.batch_size, -1)\n",
    "        else:\n",
    "            # Detach input embeddings from graph!\n",
    "            gaze_features = ze1_g_a.detach().view(self.batch_size, -1)    \n",
    "        gaze_a_hat = self.gaze2(F.relu_(self.gaze1(gaze_features)))       # gaze_1층 RELU gaze_2층임\n",
    "        gaze_a_hat = F.normalize(gaze_a_hat, dim=-1)\n",
    "\n",
    "        output_dict = {\n",
    "            'gaze_a_hat': gaze_a_hat,\n",
    "            'z_app': z_a_a,\n",
    "            'z_gaze_enc': ze1_g_a,\n",
    "            'z_head_enc': ze1_h_a,\n",
    "            'canon_z_gaze_a': self.rotate_code(data, ze1_g_a, 'gaze', fr='a'),\n",
    "            'canon_z_head_a': self.rotate_code(data, ze1_h_a, 'head', fr='a'),\n",
    "        }\n",
    "        if 'R_gaze_b' not in data:\n",
    "            return output_dict\n",
    "\n",
    "        if not is_inference_time:\n",
    "            output_dict['canon_z_gaze_b'] = self.rotate_code(data, ze1_g_b, 'gaze', fr='b')\n",
    "            output_dict['canon_z_head_b'] = self.rotate_code(data, ze1_h_b, 'head', fr='b')\n",
    "\n",
    "        # Rotate codes\n",
    "        zd1_g_b = self.rotate_code(data, ze1_g_a, 'gaze', fr='a', to='b')\n",
    "        zd1_h_b = self.rotate_code(data, ze1_h_a, 'head', fr='a', to='b')\n",
    "        output_dict['z_gaze_dec'] = zd1_g_b\n",
    "        output_dict['z_head_dec'] = zd1_h_b\n",
    "\n",
    "        # Reconstruct\n",
    "        x_b_hat = self.decode_to_image([z_a_a, zd1_g_b, zd1_h_b])\n",
    "        output_dict['image_b_hat'] = x_b_hat\n",
    "\n",
    "        # If loss functions specified, apply them\n",
    "        if loss_functions is not None:\n",
    "            losses_dict = OrderedDict()\n",
    "            for key, func in loss_functions.items():\n",
    "                losses = func(data, output_dict)  # may be dict or single value\n",
    "                if isinstance(losses, dict):\n",
    "                    for sub_key, loss in losses.items():\n",
    "                        losses_dict[key + '_' + sub_key] = loss\n",
    "                else:\n",
    "                    losses_dict[key] = losses\n",
    "            return output_dict, losses_dict\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'densenet-growthrate' : 32,\n",
    "    'z-dim_app' : 64,\n",
    "    'z-dim-gaze' : 2,\n",
    "    'z-dim-head' : 16,\n",
    "    'decoder-input-c' : 32,\n",
    "    \n",
    "    'normalize-3d-codes' : True,\n",
    "    'normalize-3d-codes-axis': 1,\n",
    "    \n",
    "    'triplet-loss-type' : 'angular',   # or euclidean\n",
    "    'triplet-loss-margin' : 0.0,\n",
    "    'triplet-regularize-d-within' : True,\n",
    "    \n",
    "    'all-equal-embeddings': True,\n",
    "    \n",
    "    'embedding-consistency-loss-type' : None, # angular, euclidean\n",
    "    'embedding-consistency-loss-warmup-samples' : 1000000,\n",
    "    \n",
    "    'backprop-gaze-to-encoder' : True,\n",
    "    \n",
    "    'coeff-l1-recon-loss' : 1.0,\n",
    "    'coeff-gaze-loss' : 0.1,\n",
    "    'coeff-embedding_consistency-loss' : 2.0, \n",
    "    \n",
    "    'pick-exactly-per-person' : None,\n",
    "    'pick-at-least-per-person' : 400,\n",
    "    \n",
    "    'use-apex' : True,\n",
    "    'base-lr': 0.0005,\n",
    "    'warmup-period-for-lr' : 1000000,\n",
    "    'batch-size' : 128,\n",
    "    'decay-interval' : 0,\n",
    "    'decay' : 0.8,\n",
    "    'num-training-epochs' : 20,\n",
    "    'l2-reg' : 1e-4,\n",
    "    'print-freq-train' : 20,\n",
    "    'print-freq-test' : 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary\n",
    "\"\"\"\n",
    "network = DTED(\n",
    "    growth_rate=param['densenet-growthrate'],\n",
    "    z_dim_app=param['z-dim_app'],\n",
    "    z_dim_gaze=param['z-dim-gaze'],\n",
    "    z_dim_head=param['z-dim-head'],\n",
    "    decoder_input_c=param['decoder-input-c'],\n",
    "    normalize_3d_codes=param['normalize-3d-codes'],\n",
    "    normalize_3d_codes_axis=param['normalize-3d-codes-axis'],\n",
    "    use_triplet=param['triplet-loss-type'],\n",
    "    backprop_gaze_to_encoder=param['backprop-gaze-to-encoder'],\n",
    ")\n",
    "\n",
    "network.to(device)\n",
    "data = torch.zeros((16, 3, 256, 256))       \n",
    "data.to(device)\n",
    "\n",
    "summary(network, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
