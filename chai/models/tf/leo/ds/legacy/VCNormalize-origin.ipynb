{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "import os, json, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_data = {\n",
    "    'iphone se': -1,\n",
    "    'iphone 4': -1,\n",
    "    'iphone 4s': {\n",
    "        'matrix': [606.59362793, 609.2008667, 236.86116028, 312.28497314],\n",
    "        'distortion': [ 0.24675941, -0.65499198,  0.00301733, -0.00097767]\n",
    "    },\n",
    "    'iphone 5': {\n",
    "        'matrix': [623.28759766, 626.64154053, 236.86317444, 316.909729  ],\n",
    "        'distortion': [ 0.03760624, -0.043609, -0.00114902,  0.00269194]\n",
    "    },\n",
    "    'iphone 5c': {\n",
    "        'matrix': [585.13171387, 588.14447021, 242.18914795, 321.20614624],\n",
    "        'distortion': [ 0.01302955, -0.10349616, -0.0009803,  0.00301618]\n",
    "    },\n",
    "    'iphone 5s': {\n",
    "        'matrix': [585.13171387, 588.14447021, 242.18914795, 321.20614624],\n",
    "        'distortion': [ 0.01302955, -0.10349616, -0.0009803,  0.00301618]\n",
    "    },\n",
    "    'iphone 6': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 6 plus': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 6s': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 6s plus': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 7': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 7 plus': {\n",
    "        'matrix': [592.50164795, 595.66986084, 236.12217712, 327.50753784],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 8': {\n",
    "        'matrix': [580.34485, 581.34717, 239.41379, 319.58548],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone 8 plus': {\n",
    "        'matrix': [580.34485, 581.34717, 239.41379, 319.58548],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone x': {\n",
    "        'matrix': [592.16473, 593.1875, 242.00687, 320.23456],\n",
    "        'distortion': [ 0.0822313, -0.18398251, -0.00631323, -0.00075782]\n",
    "    },\n",
    "    'iphone xs': -1,\n",
    "    'iphone xs max global': -1,\n",
    "    'iphone xr': -1,\n",
    "\n",
    "    'ipad air': {\n",
    "        'matrix': [578, 578, 240, 320],\n",
    "        'distortion': [0.124, -0.214, 0, 0]\n",
    "    },  # ipad air from Web\n",
    "    'ipad air 2': {\n",
    "        'matrix': [592.35223389, 595.9105835, 234.15885925, 313.48773193],\n",
    "        'distortion': [ 1.93445340e-01, -5.54507077e-01,  6.13935478e-03,  3.40262457e-04]\n",
    "    },\n",
    "    'ipad 2': {\n",
    "        'matrix': [621.54315186, 624.44012451, 233.66329956, 313.44387817],\n",
    "        'distortion': [-0.0243901, -0.10230259, -0.00513017,  0.00057966]\n",
    "    },\n",
    "    'ipad 6': -1,\n",
    "    'ipad pro 2 (10.5-inch': -1,\n",
    "\n",
    "    'ipod touch 6': -1,\n",
    "    'ipad mini': {\n",
    "        'matrix': [623.28759766, 626.64154053, 236.86317444, 316.909729],\n",
    "        'distortion': [ 0.03760624, -0.043609, -0.00114902,  0.00269194]\n",
    "    },\n",
    "}\n",
    "debug_print = False\n",
    "normalize_distance = 300\n",
    "\n",
    "normalize_face_focal_length = 200\n",
    "normalize_face_size = (96, 48)\n",
    "\n",
    "normalize_eye_focal_length = 700\n",
    "normalize_eye_size = (96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_norm = np.array([\n",
    "    [normalize_face_focal_length, 0, normalize_face_size[0] / 2],\n",
    "    [0, normalize_face_focal_length, normalize_face_size[1] / 2],\n",
    "    [0, 0, 1.0],\n",
    "])\n",
    "\n",
    "eye_norm = np.array([\n",
    "    [normalize_eye_focal_length, 0, normalize_eye_size[0] / 2],\n",
    "    [0, normalize_eye_focal_length, normalize_eye_size[1] / 2],\n",
    "    [0, 0, 1.0],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_output = '/mnt/sata3/everyone-tfrecord2'\n",
    "dir_input = '/mnt/sata4/everyone'\n",
    "image_dir = 'frames'\n",
    "json_dir = 'data'\n",
    "landmarkd_dir = 'landmark2'\n",
    "tfrecord_dir = 'tfrecord2'\n",
    "temptfrecord_dir = 'temp_record'\n",
    "\n",
    "phases = ['train', 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFocalLengthAndDistortion(device_name):\n",
    "    device_name = device_name.lower()\n",
    "\n",
    "    if not(device_name in device_data):\n",
    "        return None, None\n",
    "\n",
    "    if device_data[device_name] == -1:\n",
    "        return None, None\n",
    "\n",
    "    return np.array(device_data[device_name]['matrix']), np.array(device_data[device_name]['distortion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutImage(image, landmarks):\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "\n",
    "    min_x = max(int(min(landmarks[:, 0])), 0)\n",
    "    max_x = min(int(max(landmarks[:, 0])), width)\n",
    "    min_y = max(int(min(landmarks[:, 1])), 0)\n",
    "    max_y = min(int(max(landmarks[:, 1])), height)\n",
    "    \n",
    "#     print('minmax', min_y, max_y)\n",
    "    w = max_x - min_x\n",
    "    h = max_y - min_y\n",
    "\n",
    "    img_xs = int(max(min_x - 0.4 * w, 0))\n",
    "    img_xe = int(min(max_x + 0.4 * w, width))\n",
    "    img_ys = int(max(min_y - 0.1 * h, 0))\n",
    "    img_ye = int(min(max_y + 0.0 * h, height))\n",
    "\n",
    "    # if jaws are removed\n",
    "    if len(landmarks) == 17:\n",
    "        img_ys = int(max(min_y - 0.3 * h, 0))\n",
    "        img_ye = int(min(max_y + 1.0 * h, height))\n",
    "        \n",
    "#     print ('image:', image.shape, ',', img_ys, ',', img_ye, ',', img_xs, ',', img_xe)\n",
    "    return image[img_ys:img_ye, img_xs:img_xe], landmarks - [img_xs, img_ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawImage(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def drawCVImage(image):\n",
    "    cv2.circle(image, tuple([48, 48]), 2, (0, 0, 255), -1)\n",
    "    image = image[:,:,::-1]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def drawLandmarks(image, landmarks):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for index, pt in enumerate(landmarks):\n",
    "        cv2.circle(image, tuple([pt[0].astype(int), pt[1].astype(int)]), 1, (0, 0, 255), -1)\n",
    "#         cv2.putText(image, str(index), tuple([(pt[0] + 2).astype(int), (pt[1] - 2).astype(int)]), font, 0.3, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "    image = image[:,:,::-1]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_landmark_to_classic(landmarks):\n",
    "    landmark_map = [7, 8, 9, \n",
    "                    27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "                    36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
    "    classic = np.zeros((68, 2))\n",
    "    for i, idx in enumerate(landmark_map):\n",
    "        classic[idx] = landmarks[i]\n",
    "    return classic\n",
    "\n",
    "def pickEffectiveLandmarks(raw_landmarks):\n",
    "    jaws = raw_landmarks[7:10]\n",
    "    eye_right = raw_landmarks[36:42]\n",
    "    eye_left = raw_landmarks[42:48]\n",
    "    nose_bridge = raw_landmarks[27:31]\n",
    "    nose_bottom = raw_landmarks[33:34]\n",
    "\n",
    "    return np.vstack((eye_right, eye_left, nose_bridge, nose_bottom, jaws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStandard3DFacePoints():\n",
    "    faceModel = 'standard3DFace.json'\n",
    "\n",
    "    if not os.path.isfile(faceModel):\n",
    "        print(faceModel, 'not found!')\n",
    "        return None\n",
    "\n",
    "    with open(faceModel, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    eye_right   = np.array( data['eye_right'],   dtype='float32')\n",
    "    eye_left    = np.array( data['eye_left'],    dtype='float32')\n",
    "    nose        = np.array( data['nose'],        dtype='float32')\n",
    "    nose_bottom = np.array( data['nose_bottom'], dtype='float32')\n",
    "    jaws        = np.array( data['jaws'],        dtype='float32')\n",
    "\n",
    "    return np.vstack((eye_right, eye_left, nose, nose_bottom, jaws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "def estimateHeadPosition(refined_landmarks, position, camera_matrix, camera_distortion):\n",
    "#     print('refined_landmarks:', refined_landmarks, 'position:',position, 'camera_matrix:',camera_matrix, 'camera_distortion:', camera_distortion)\n",
    "    ret, rvec, tvec = cv2.solvePnP(position, refined_landmarks,\n",
    "                                   camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_EPNP)\n",
    "    ret, rvec, tvec = cv2.solvePnP(position, refined_landmarks, camera_matrix, camera_distortion, rvec, tvec, True)\n",
    "\n",
    "    return rvec, tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plane(p1, p2, p3):\n",
    "    # These two vectors are in the plane\n",
    "    v1 = p3 - p1\n",
    "    v2 = p2 - p1\n",
    "\n",
    "    # the cross product is a vector normal to the plane\n",
    "    cp = np.cross(v1, v2)\n",
    "    a, b, c = cp\n",
    "\n",
    "    # This evaluates a * x3 + b * y3 + c * z3 which equals d\n",
    "    d = np.dot(cp, p3)\n",
    "\n",
    "    # The equation is ax + by + cz = d\n",
    "    # But we want to have ax + by + c = z\n",
    "#     print('The equation is {0}x + {1}y + {2}z = {3}'.format(a, b, c, d))\n",
    "    a = - a / c\n",
    "    b = - b / c\n",
    "    c = d / c\n",
    "    return a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R(rotation_matrix, center):\n",
    "    distance = np.linalg.norm(center)\n",
    "    hRx = rotation_matrix[:, 0]\n",
    "    \n",
    "    forward = (center / distance).reshape(3)\n",
    "    \n",
    "    down = np.cross(forward, hRx)\n",
    "    down /= np.linalg.norm(down)\n",
    "    \n",
    "    right = np.cross(down, forward)\n",
    "    right /= np.linalg.norm(right)\n",
    "    \n",
    "    return np.c_[right, down, forward].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We need modified S that does not scale but move Z by k distance\n",
    "    make plane that formed by le, re, nose_tip\n",
    "    ax + by + c = z\n",
    "    S = |   1      0      0  |\n",
    "        |   0      1      0  |\n",
    "        | -ak/c  -bk/c  1+k/c|\n",
    "\n",
    "    S moves (x, y, z) to (x, y, z + k)\n",
    "'''\n",
    "\n",
    "def calculate_S(pt1, pt2, pt3, R, target_distance):\n",
    "    pt1 = np.dot(R, pt1)\n",
    "    pt2 = np.dot(R, pt2)\n",
    "    pt3 = np.dot(R, pt3)\n",
    "    \n",
    "    p1 = np.reshape(pt1, 3)\n",
    "    p2 = np.reshape(pt2, 3)\n",
    "    p3 = np.reshape(pt3, 3)\n",
    "    \n",
    "    a, b, c = get_plane(p1, p2, p3)\n",
    "    k = target_distance - c\n",
    "    \n",
    "    S = np.array([\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [-a*k/c, -b*k/c, 1+k/c],\n",
    "    ])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLandmarksFromJson(data):\n",
    "    if 'landmark2' in data:\n",
    "        jsonarr = np.array(data['landmark2'])\n",
    "        jsonarr = jsonarr.reshape([-1, 2])\n",
    "        return pickEffectiveLandmarks(modify_landmark_to_classic(jsonarr))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutsideImage(image, landmarks, face3d):\n",
    "    # removing jaws only\n",
    "    isoutside = False\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "\n",
    "    for i in range(17, 20):\n",
    "        if landmarks[i][0] > width * 0.98 or landmarks[i][1] > height * 0.98 or landmarks[i][0] < width * 0.02 or landmarks[i][1] < height * 0.02:\n",
    "            isoutside = True\n",
    "            break\n",
    "\n",
    "#     if isoutside:\n",
    "    return landmarks[0:17], face3d[0:17]\n",
    "#     else:\n",
    "#         return landmarks, face3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardFace = getStandard3DFacePoints()\n",
    "if standardFace is None:\n",
    "    print('Facemodel file not found!')\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(json_path, image_path):\n",
    "\n",
    "    # load image and data files\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(image_path, 'not exists!')\n",
    "        return None\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "#     with open(landmark_path, 'r') as f:\n",
    "#         landmark_data = json.load(f)\n",
    "\n",
    "\n",
    "    params, dists = getFocalLengthAndDistortion(data['deviceName'])\n",
    "    if params is None:\n",
    "        return None\n",
    "\n",
    "    fx, fy = params[0], params[1]\n",
    "    if image.shape[1] > image.shape[0]:  # width > height\n",
    "        cx, cy = params[3], params[2]\n",
    "    else:\n",
    "        cx, cy = params[2], params[3]\n",
    "\n",
    "    camera_distortion = np.hstack((dists, 0))\n",
    "\n",
    "    camera_matrix = np.array([\n",
    "        [fx,  0, cx],\n",
    "        [ 0, fy, cy],\n",
    "        [ 0,  0, 1 ]\n",
    "    ])\n",
    "\n",
    "    landmarks = getLandmarksFromJson(data)\n",
    "\n",
    "    if landmarks is None:\n",
    "        print(image_path, 'has no landmark information!')\n",
    "        return None\n",
    "\n",
    "    landmarks, face_3d = removeOutsideImage(image, landmarks, copy.deepcopy(standardFace))\n",
    "    # make to 3-D array\n",
    "    landmarks = landmarks.reshape(-1, 1, 2)\n",
    "    face_3d = face_3d.reshape(-1, 1, 3)\n",
    "\n",
    "    lookat = np.array([-data['XCam'], -data['YCam'], 0])\n",
    "    # cm to mm\n",
    "    lookat = lookat * 10\n",
    "    lookat = lookat.reshape((3, 1))\n",
    "    \n",
    "    return image, camera_matrix, camera_distortion, landmarks, face_3d, lookat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataAndGaze(image, face, camera_matrix, look_vector, head_rotate, landmarks):\n",
    "    # normalizing face area\n",
    "    nose_tip = face[:, 15].reshape((3, 1))\n",
    "    eye_center = np.array([sum(x) for x in face[:, 0:12]]) / 12\n",
    "    re = np.array([sum(x) for x in face[:, 0:6]]) / 6\n",
    "    le = np.array([sum(x) for x in face[:, 6:12]]) / 6\n",
    "\n",
    "    center = eye_center.reshape((3, 1))\n",
    "    \n",
    "    gaze_data = []\n",
    "    warped_image = []\n",
    "    R_list = []\n",
    "    for eye in [re, le]:\n",
    "        R = get_R(head_rotate, eye)\n",
    "        S = calculate_S(re, le, nose_tip, R, normalize_distance)\n",
    "        W = np.dot(np.dot(eye_norm, S), np.dot(R, np.linalg.inv(camera_matrix)))  # transformation matrix\n",
    "        image_warped = cv2.warpPerspective(image, W, normalize_eye_size)  # image normalization\n",
    "\n",
    "        eye = eye.reshape((3, 1))\n",
    "        g = look_vector - eye\n",
    "        g = np.dot(R, g)\n",
    "        g = g / (-g[2])\n",
    "        \n",
    "        warped_image.append(image_warped)\n",
    "        gaze_data.append(g)\n",
    "        R_list.append(R)\n",
    "        if debug_print:\n",
    "            drawCVImage(image_warped)\n",
    "    \n",
    "    center = (nose_tip + eye_center.reshape((3, 1))) / 2\n",
    "    R = get_R(head_rotate, center)\n",
    "    S = calculate_S(re, le, nose_tip, R, normalize_distance)\n",
    "    W = np.dot(np.dot(face_norm, S), np.dot(R, np.linalg.inv(camera_matrix)))  # transformation matrix\n",
    "    image_warped = cv2.warpPerspective(image, W, normalize_face_size)  # image normalization\n",
    "    transformed_lks = cv2.perspectiveTransform(landmarks, W)\n",
    "\n",
    "    warped_image.append(image_warped)\n",
    "    R_list.append(R)\n",
    "    if debug_print:\n",
    "        drawLandmarks(image_warped, transformed_lks.reshape((-1, 2)))\n",
    "\n",
    "#     right_gaze = gaze_data[0]\n",
    "#     right_eye = eye_right.reshape((3, 1))\n",
    "#     original_right_gaze = np.dot(np.linalg.inv(R), right_gaze)\n",
    "#     multiplier = - np.divide(right_eye[2], original_right_gaze[2])\n",
    "#     target = np.add(right_eye, np.multiply(original_right_gaze, multiplier))\n",
    "#     print('right_gaze:', right_gaze, ', right_eye:', right_eye, ', target(re):', target, ', target(true)', look_vector)\n",
    "    \n",
    "    return warped_image, np.array(gaze_data), R_list #cv2.Rodrigues(np.dot(R, hR))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_normalize(subject, frame):\n",
    "    name = subject + '_' + frame\n",
    "    json_path = os.path.join(input_json_dir, subject, name) + '.json'\n",
    "    image_path = os.path.join(input_image_dir, subject, name) + '.jpg'\n",
    "#     landmark_path = os.path.join(input_landmark_dir, data_name) + '.json'\n",
    "    \n",
    "    try:\n",
    "        image, camera_matrix, camera_distortion, landmarks, face_3d, lookat = read_data(json_path, image_path)\n",
    "    except:\n",
    "        print('Failed to read data : ', data_name)\n",
    "        return None\n",
    "    \n",
    "    # undistort landmark points and image\n",
    "    landmarks = cv2.undistortPoints(landmarks, camera_matrix, camera_distortion, P=camera_matrix)\n",
    "    image_undistorted = cv2.undistort(image, camera_matrix, camera_distortion)\n",
    "    \n",
    "#     drawCVImage(image)\n",
    "    # pnp R and T\n",
    "    hr, ht = estimateHeadPosition(landmarks, face_3d, camera_matrix, camera_distortion)\n",
    "    face_3d = face_3d.reshape(-1, 3).T\n",
    "    ht = ht.reshape((3, 1))\n",
    "\n",
    "    # Rodrigues expression to 3x3 rotation matrix\n",
    "    hR = cv2.Rodrigues(hr)[0]  # rotation matrix\n",
    "    translate = ht.reshape((3, 1))\n",
    "    face = np.dot(hR, face_3d) + translate  # 3D positions of facial landmarks\n",
    "\n",
    "    # warped image, rotated gaze vector, face R, S, W, 3D face points\n",
    "    image_warped, gaze_vector, R = normalizeDataAndGaze(image_undistorted, \n",
    "                                                        face,\n",
    "                                                        camera_matrix,\n",
    "                                                        lookat,\n",
    "                                                        hR,\n",
    "                                                        landmarks)\n",
    "\n",
    "\n",
    "    eye_right = np.array([sum(x) for x in face[:, 0:6]]) / 6\n",
    "    eye_left = np.array([sum(x) for x in face[:, 6:12]]) / 6\n",
    "    \n",
    "#     if abs(hR[2,0]) + abs(hR[2,1]) > abs(hR[2,2]) or hR[2,2]< 0:\n",
    "#         print(hR)\n",
    "#         drawCVImage(image)\n",
    "#         drawLandmarks(image_undistorted, np.reshape(landmarks, [-1, 2]))\n",
    "    \n",
    "    right_eye_pose = cv2.Rodrigues(np.dot(R[0], hR))[0]\n",
    "    left_eye_pose = cv2.Rodrigues(np.dot(R[1], hR))[0]\n",
    "    face_pose = cv2.Rodrigues(np.dot(R[2], hR))[0]\n",
    "    \n",
    "    \n",
    "    eyes = np.append(eye_right, eye_left)\n",
    "    poses = np.reshape(np.concatenate((right_eye_pose, left_eye_pose, face_pose), axis=0), (-1))\n",
    "\n",
    "#     if out_print:\n",
    "#         print(record_data['gaze_pixel'])\n",
    "#         drawCVImage(image)\n",
    "#         print(record_data['orientation'])\n",
    "#         print(eyes)    \n",
    "#         print(eye_poses)\n",
    "\n",
    "    return image_warped, gaze_vector, eyes, poses, R[0], R[1], R[2], lookat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_print:\n",
    "    phase = phases[0]\n",
    "    input_dir = os.path.join(dir_input, phase)\n",
    "    output_dir = os.path.join(dir_output, phase)\n",
    "    input_image_dir = os.path.join(input_dir, image_dir)\n",
    "    input_json_dir = os.path.join(input_dir, json_dir)\n",
    "    input_landmark_dir = os.path.join(input_dir, landmarkd_dir)\n",
    "    \n",
    "    with open(os.path.join(input_dir, 'info_normalized2.json'), 'r') as f:\n",
    "        data = json.load(f)['subjects']\n",
    "\n",
    "    start_subject = 50\n",
    "    end_subject = 70\n",
    "\n",
    "    subject = list(data.keys())[0]\n",
    "    count = 0\n",
    "    for subject in data.keys():\n",
    "        count += 1\n",
    "        if count < start_subject:\n",
    "            continue\n",
    "        for frame in data[subject]:\n",
    "            data_name = subject + '_' + frame\n",
    "            image_warped, gaze_vector, eyes, eye_poses, right_inv_R, left_inv_R, lookat = do_normalize(subject, frame)\n",
    "            print(eyes)\n",
    "            print(eye_poses)\n",
    "            break\n",
    "\n",
    "        if (count >= end_subject):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _bytes_feature2(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def make_example(subject_name, frame_name, re, le, we, gaze, eyes, poses, rR, lR, cR, gaze2d):\n",
    "    feature = {\n",
    "        'subject': _bytes_feature(subject_name),\n",
    "        'frame': _bytes_feature(frame_name),\n",
    "        'img_re': _bytes_feature(re),\n",
    "        'img_le': _bytes_feature(le),\n",
    "        'img_we': _bytes_feature(we),\n",
    "        'gaze': _float_feature(np.reshape(gaze, (-1)).tolist()),\n",
    "        'eyes': _float_feature(np.reshape(eyes, (-1)).tolist()),\n",
    "        'poses': _float_feature(np.reshape(poses, (-1)).tolist()),\n",
    "        'rR': _float_feature(np.reshape(rR, (-1)).tolist()),\n",
    "        'lR': _float_feature(np.reshape(lR, (-1)).tolist()),\n",
    "        'cR': _float_feature(np.reshape(cR, (-1)).tolist()),\n",
    "        'gaze2d': _float_feature(np.reshape(gaze2d, (-1)).tolist()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def _parse_image_function(example_raw):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_raw, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(writer, subject, frame, image_warped, gaze, eyes, poses, rR, lR, cR, gaze2d):\n",
    "    subject_name = bytes(str(subject), encoding='ascii')\n",
    "    frame_name = bytes(str(frame), encoding='ascii')\n",
    "\n",
    "    re = image_warped[0]\n",
    "    le = image_warped[1]\n",
    "    we = image_warped[2]\n",
    "    \n",
    "    try:\n",
    "\n",
    "#         re = cv2.resize(re, (96, 64))\n",
    "    #     drawImage(re)\n",
    "        re = tf.io.encode_jpeg(re, quality=100).numpy()\n",
    "\n",
    "#         le = cv2.resize(le, (96, 64))\n",
    "    #     drawImage(le)\n",
    "        le = tf.io.encode_jpeg(le, quality=100).numpy()\n",
    "\n",
    "#         we = cv2.resize(we, (96, 48))\n",
    "    #     drawImage(we)            \n",
    "        we = tf.io.encode_jpeg(we, quality=100).numpy()\n",
    "\n",
    "        example = make_example(subject_name, frame_name, re, le, we, gaze, eyes, poses, rR, lR, cR, gaze2d)\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "    except:\n",
    "        print(\"Failed to make example: \", subject, \"-\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_subject(subject):\n",
    "    temp_file = os.path.join(temp_out_dir, subject + '.tfrecords')\n",
    "    dst_file = os.path.join(out_tfrecord_dir, subject + '.tfrecords')\n",
    "    \n",
    "    if os.path.isfile(dst_file):\n",
    "        return\n",
    "    \n",
    "    with tf.io.TFRecordWriter(temp_file) as writer:\n",
    "        for frame in data[subject]:\n",
    "            data_name = subject + '_' + frame\n",
    "#             try:\n",
    "            output = do_normalize(subject, frame)\n",
    "            if output is None:\n",
    "                continue\n",
    "            [image_warped, gaze, eyes, poses, rR, lR, cR, gaze2d] = output\n",
    "            write_tfrecord(writer, subject, frame, image_warped, gaze, eyes, poses, rR, lR, cR, gaze2d)\n",
    "#             except:\n",
    "#                 print(\"Failed frame : \", data_name)\n",
    "    shutil.move(temp_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [5:59:32<00:00, 17.58s/it]  \n",
      "100%|██████████| 50/50 [14:27<00:00, 17.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for phase in phases:\n",
    "    input_dir = os.path.join(dir_input, phase)\n",
    "    output_dir = os.path.join(dir_output, phase)\n",
    "    input_image_dir = os.path.join(input_dir, image_dir)\n",
    "    input_json_dir = os.path.join(input_dir, json_dir)\n",
    "    input_landmark_dir = os.path.join(input_dir, landmarkd_dir)\n",
    "    out_tfrecord_dir = os.path.join(output_dir, tfrecord_dir)\n",
    "    temp_out_dir = os.path.join(output_dir, temptfrecord_dir)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    if not os.path.exists(out_tfrecord_dir):\n",
    "        os.mkdir(out_tfrecord_dir)    \n",
    "    if not os.path.exists(temp_out_dir):\n",
    "        os.mkdir(temp_out_dir)\n",
    "        \n",
    "    with open(os.path.join(input_dir, 'info_normalized2.json'), 'r') as f:\n",
    "        data = json.load(f)['subjects']\n",
    "    \n",
    "    keys = list(data.keys())\n",
    "    keys.reverse()\n",
    "    pool = Pool(4)\n",
    "    for _ in tqdm(pool.imap_unordered(preprocess_one_subject, keys), total=len(keys)):\n",
    "        pass\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "#     for subject in data.keys():\n",
    "#         preprocess_one_subject(subject)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
